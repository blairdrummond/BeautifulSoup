from bs4 import BeautifulSoup
import urllib.parse, urllib.request, re, time, random, math

# NOTE THAT

# In order to supplement the missing data, we have added data/ manipulated data to fill everything

# The store manager is always 'Bill Clinton'
# The store was first opened on Bill Clinton's birthday
# The rating from urban spoon is randomly turned into a split of 4 traits with that average


# Cache values in these lists then flush them once everything is done.
restKeys     = []
ratiURLs     = []
restSet      = []
locaSet      = []
featSet      = []
cuisSet      = []
menuSet      = []
criticSet    = []
reviewerSet  = []
cratingSet   = []
rratingSet   = []
criticData   = set([])
reviewerData = set([])

# Extract data from the hours tag on a restaurant
week   = ['Monday','Tuesday','Wednesday','Thursday','Friday','Saturday','Sunday'] 

months = { 
    'January'   : '01', 
    'February'  : '02',
    'March'     : '03',
    'April'     : '04',
    'May'       : '05',
    'June'      : '06',
    'July'      : '07',
    'August'    : '08',
    'September' : '09',
    'October'   : '10',
    'November'  : '11',
    'December'  : '12'
}

numParse = re.compile('(?<=/r/\d\d\d/)\d+')





def parseHours(hourstag):
    # reformat text like 'Monday - Friday' to 'Monday'
    days    = [ x.text.split()[0] for x in hourstag.find_all('h4')]
    hours   = [ x.text            for x in hourstag.find_all('p')]
    pairs = list(zip(days, hours))

    # For every day of the week except Monday...
    for i in range(1,7):
        # if a day is missing
        if len(pairs) <= i or pairs[i][0] != week[i]:
            # insert that day based on the previous day's hours
            pairs.insert( i,  (week[i], pairs[i-1][1])  )    

    return pairs

# Extract data from he address tag on a restaurant
def parseAddress(addrtag):
    # Switch-Block for assigning the relevant variables
    for x in addrtag.find_all('span'):
        if 'class' in x.attrs:
            if   'street-address' in x['class']:
                address  = x.text.strip()
            elif 'locality' in x['class']:
                cityProv   = x.text.replace('\n',' ').replace(',','').split()
                city, prov = cityProv[0], cityProv[1]
            elif 'visible-xs-inline' in x['class']:
                postal   = x.text.strip()

    return address, city, prov, postal



def parseMenu(index, url)
    page = urllib.request.urlopen(url)
    soup = BeautifulSoup(page)
    return [ (index, x.img['src'], x.h3.a.text) for x in soup.find_all('li', {'data-class' : 'Dish'})] 


def genRatings(average):
    ratings = []
    for i in range(0,4):
        x = max( 0, min( 100, round(random.gauss( average, 10 ))))  # Normal variable between 0 and 100
        ratings.insert( 0,  x )

    return ratings[0], ratings[1], ratings[2], ratings[3]


def grabPage(url):
    page = urllib.request.urlopen(url)
    soup = BeautifulSoup(page)

    # Grab a Whole bunch of stuff
    name          = soup.find('meta', { "property"       : "og:title"                })['content']
    index         = soup.body['data-restaurant-id']
    imageURL      = soup.find('meta', { "property"       : "og:image"                })['content']
    neighbourhood = soup.find('meta', { "property"       : "urbanspoon:neighborhood" })['content']
    ratingPerc    = soup.find('meta', { "property"       : "urbanspoon:like_percent" })['content']
    website       = soup.find('div',  { "data-ga-action" : "resto-website"           }).a['href']
    menusite      = soup.find('a',    { "data-ga-action" : "menu-urbanspoon"         })['href']
    cuisines      = [ tag['content'] for tag in soup.find_all('meta', { "property"       : "urbanspoon:cuisine" }) ]
    features      = [ tag.text       for tag in soup.find_all('a',    { "data-ga-action" : "resto-feature"      }) ]
    phone         = soup.find('a', 'phone tel').text.strip()
    cost          = 4 - len(soup.find('a', { "data-ga-action" : "resto-price" }).span.text)  # OUT OF 4
    weeklyhours   = parseHours( soup.find('div', { "data-in-base-append" : "#hours-base" }) )
    address, city, prov, postal = parseAddress(soup.find('div', id='address'))


    # Create Rows
    foodra, pricra, moodra, stafra = genRatings(rating)

    restSet.insert(0, (index, name, cost, foodra, pricra, moodra, stafra, website))
    
    # Useful for keeping track of something else later.
    restKeys.insert(0,index)

    locationindex = len([ x for x in locaSet if x[-1] == index ])
    locaSet.insert( 0, 
                    (  
                        locationindex, 
                        'NULL', 
                        '1948-08-19',
                        'Bill Clinton',
                        phone, 
                        address, 
                        imageURL,
                        weeklyhours[0][1], 
                        weeklyhours[1][1], 
                        weeklyhours[2][1], 
                        weeklyhours[3][1], 
                        weeklyhours[4][1], 
                        weeklyhours[5][1], 
                        weeklyhours[6][1], 
                        index           
                    ))

    for cuisine in cuisines:
        cuisSet.insert(0, (cuisine, index))
    
    for feature in features:
        featSet.insert(0, (feature, index))

    menulist = parsemenu( index, 'www.urbanspoon.com' + menusite  )
    for menuitem in menulist:
        menuSet.inster(0, menuitem)




    # AFTER all restaraunts are loaded, find all the critics/reviewers.
    # The urls are found here so that they can be referenced later.
    # Critics prone to repeats
    reviewers = [ x['href'] for x in soup.find_all('a', {'itemprop' : 'reviewer'})]
    critics   = set([ x['href'] for x in soup.find_all('a', {'data-ga-action' : 'critic-page'})])
    
    for e in reviewers:
        reviewerData.add(e)
    for e in critics:
        criticData.add(e)
    # DONE



def grabReviewer():
    i = 0

    for e in reviewerData:
        page  = urllib.request.urlopen( 'http://www.urbanspoon.com' + e )
        soup  = BeautifulSoup(page)
        temp  = soup.find('header', {'class' : 'page-header'}).text.replace('\n',' ').split()
        name  = temp[0]
        month = months[temp[3]]
        day   = (lambda x: [0]+x if (len(x) == 1) else x )(temp[4].replace(',',''))
        year  = temp[5]
        joind = year +'-'+ month +'-'+ day 
        city  = temp[-1]

        image = [ x.img['src'] for x in soup.find_all('a', { 'data-ga-action' : 'user-profile-page' }) if x.img != None][0]

        numreviews = int(re.search('\d+',  [li.text.replace('\n','') for li in soup.find_all('li') if li.a != None and li.a.text == 'Reviews' ][0]  ).group(0))

        calcdrating = min( 5, max(1 , math.floor(math.log( max(numreviews, 1), 2) - 1)))
        
        reviewerSet.insert(0,    (index, name+'@foodie.ca', 'password', name, joind, 'reviewer', calcdrating ))

        addComments(index,  'http://www.urbanspoon.com' + e + '/comments' )


def revRating(index, url):
    page  = urllib.request.urlopen( 'http://www.urbanspoon.com' + e )
    soup  = BeautifulSoup(page)

    for x in [ x for x in soup.find_all('li', { 'class' : 'comment review'}) ]:
        rating   = x['data-positive'] # 0 < x < 10, x in R
        date     = soup.find('time')['datetime'][:10]
        restNum  = int(numParse.search( x.a['href']).group(0))
        revTitle = x.find('div', { 'class' : 'subtitle' }).text
        comment  = x.find('div', { 'class' : 'body'     }).text.strip()
        foodra, pricra, moodra, stafra = genRatings(rating)        
        rratingSet.insert(0,    (index, date,  foodra, pricra, moodra, stafra, revTitle, comment, restNum)       )



def grabCritic(url):

def criRating(index, url):


#FIND THE URLS TO THE RESTAURANTS
#output = open('sites.txt','w')
#
#for i in range(1, 182): 
#    url='http://www.urbanspoon.com/lb/250/best-restaurants-Ottawa?page='+str(i)  #input('URL:')
#    print (i)
#    page = urllib.request.urlopen(url)
#    soup = BeautifulSoup(page)
#    
#    links1 =  soup.find_all('a','resto_name')
#    links2 =  [ tag['href'] for tag in links1 ]
#
#    for link in links2:
#        output.write(link + '\n')
#
#output.close()


with open('sites.txt','r') as f:
    sites = f.readlines()

index = 0
for url in sites:
    grabPage('http://www.urbanspoon.com' + url )
    
grabCritics()
grabReviewers()


